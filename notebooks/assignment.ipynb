{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbd279e",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "## Brief\n",
    "\n",
    "Write the Python codes for the following questions.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Paste the answer as Python in the answer code section below each question.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Question: The scraping of `https://www.scrapethissite.com/pages/forms/` in the last section assumes a hardcoded (fixed) no of pages. Can you improve the code by removing the hardcoded no of pages and instead use the `»` button to determine if there are more pages to scrape? Hint: Use a `while` loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7f050c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dynamic hockey team scraping...\n",
      "Scraping page 1...\n",
      "Found 25 teams on page 1\n",
      "Scraping page 2...\n",
      "Found 25 teams on page 2\n",
      "Scraping page 3...\n",
      "Found 25 teams on page 3\n",
      "Scraping page 4...\n",
      "Found 25 teams on page 4\n",
      "Scraping page 5...\n",
      "Found 25 teams on page 5\n",
      "Scraping page 6...\n",
      "Found 25 teams on page 6\n",
      "Scraping page 7...\n",
      "Found 25 teams on page 7\n",
      "Scraping page 8...\n",
      "Found 25 teams on page 8\n",
      "Scraping page 9...\n",
      "Found 25 teams on page 9\n",
      "Scraping page 10...\n",
      "Found 25 teams on page 10\n",
      "Scraping page 11...\n",
      "Found 25 teams on page 11\n",
      "Scraping page 12...\n",
      "Found 25 teams on page 12\n",
      "Scraping page 13...\n",
      "Found 25 teams on page 13\n",
      "Scraping page 14...\n",
      "Found 25 teams on page 14\n",
      "Scraping page 15...\n",
      "Found 25 teams on page 15\n",
      "Scraping page 16...\n",
      "Found 25 teams on page 16\n",
      "Scraping page 17...\n",
      "Found 25 teams on page 17\n",
      "Scraping page 18...\n",
      "Found 25 teams on page 18\n",
      "Scraping page 19...\n",
      "Found 25 teams on page 19\n",
      "Scraping page 20...\n",
      "Found 25 teams on page 20\n",
      "Scraping page 21...\n",
      "Found 25 teams on page 21\n",
      "Scraping page 22...\n",
      "Found 25 teams on page 22\n",
      "Scraping page 23...\n",
      "Found 25 teams on page 23\n",
      "Scraping page 24...\n",
      "Found 7 teams on page 24\n",
      "No more pages found (no '»' button). Scraping complete!\n",
      "\n",
      "Scraping complete! Found 582 total teams.\n",
      "DataFrame shape: (582, 9)\n",
      "\n",
      "First few rows:\n",
      "            Team Name  Year Wins Losses OT Losses  Win % Goals For (GF)  \\\n",
      "0       Boston Bruins  1990   44     24             0.55            299   \n",
      "1      Buffalo Sabres  1990   31     30            0.388            292   \n",
      "2      Calgary Flames  1990   46     26            0.575            344   \n",
      "3  Chicago Blackhawks  1990   49     23            0.613            284   \n",
      "4   Detroit Red Wings  1990   34     38            0.425            273   \n",
      "\n",
      "  Goals Against (GA) + / -  \n",
      "0                264    35  \n",
      "1                278    14  \n",
      "2                263    81  \n",
      "3                211    73  \n",
      "4                298   -25  \n",
      "\n",
      "Data saved to 'hockey_teams_all_pages.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def parse_and_extract_rows(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Extract table rows from the parsed HTML.\n",
    "    \n",
    "    Args:\n",
    "        soup: The parsed HTML.\n",
    "    \n",
    "    Returns:\n",
    "        An iterator of dictionaries with the data from the current page.\n",
    "    \"\"\"\n",
    "    header = soup.find('tr')\n",
    "    headers = [th.text.strip() for th in header.find_all('th')]\n",
    "    teams = soup.find_all('tr', 'team')\n",
    "    for team in teams:\n",
    "        row_dict = {}\n",
    "        for header, col in zip(headers, team.find_all('td')):\n",
    "            row_dict[header] = col.text.strip()\n",
    "        yield row_dict\n",
    "\n",
    "def has_next_page(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Check if there's a next page by looking for the '»' button.\n",
    "    \n",
    "    Args:\n",
    "        soup: The parsed HTML.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if next page exists, False otherwise.\n",
    "    \"\"\"\n",
    "    # Look for the '»' symbol in pagination\n",
    "    next_button = soup.find('a', {'aria-label': 'Next'})\n",
    "    if next_button:\n",
    "        return '»' in next_button.text\n",
    "    \n",
    "    # Alternative: Look for any link containing '»'\n",
    "    next_links = soup.find_all('a', string='»')\n",
    "    return len(next_links) > 0\n",
    "\n",
    "def scrape_all_hockey_pages(base_url=\"https://www.scrapethissite.com/pages/forms/\"):\n",
    "    \"\"\"\n",
    "    Scrape all pages of hockey team data using dynamic pagination detection.\n",
    "    \n",
    "    Args:\n",
    "        base_url: The base URL for the hockey teams page.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing all team data.\n",
    "    \"\"\"\n",
    "    all_teams = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        \n",
    "        # Construct URL for current page\n",
    "        url = f\"{base_url}?page_num={page}\"\n",
    "        \n",
    "        # Make request\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            r.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching page {page}: {e}\")\n",
    "            break\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        \n",
    "        # Extract data from current page\n",
    "        page_teams = list(parse_and_extract_rows(soup))\n",
    "        \n",
    "        # If no teams found on this page, we've gone too far\n",
    "        if not page_teams:\n",
    "            print(f\"No data found on page {page}. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        # Add teams from this page to our collection\n",
    "        all_teams.extend(page_teams)\n",
    "        print(f\"Found {len(page_teams)} teams on page {page}\")\n",
    "        \n",
    "        # Check if there's a next page using the '»' button\n",
    "        if not has_next_page(soup):\n",
    "            print(\"No more pages found (no '»' button). Scraping complete!\")\n",
    "            break\n",
    "        \n",
    "        # Move to next page\n",
    "        page += 1\n",
    "        \n",
    "        # Be respectful - add a small delay between requests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return all_teams\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting dynamic hockey team scraping...\")\n",
    "    \n",
    "    # Scrape all pages\n",
    "    hockey_data = scrape_all_hockey_pages()\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nScraping complete! Found {len(hockey_data)} total teams.\")\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(hockey_data)\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Optional: Save to CSV\n",
    "    df.to_csv(\"hockey_teams_all_pages.csv\", index=False)\n",
    "    print(\"\\nData saved to 'hockey_teams_all_pages.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
